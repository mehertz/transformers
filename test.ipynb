{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from train import GPT\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 50257])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "gpt = GPT(\n",
    "    vocab_size=50257,\n",
    "    context_length=1024,\n",
    "    emb_dim=768,\n",
    "    ff_int_dim_mult=4,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    drop_rate=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "out = gpt(torch.randint(low=0, high=50257, size=(2, 5)))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4342, 318, 617, 2420, 50256, 3549, 2420]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "tokenizer.decode([50256])\n",
    "\n",
    "text = \"Here is some text<|endoftext|>more text\"\n",
    "tokenizer.encode(text, allowed_special={'<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+------------+\n",
      "|                Modules                 | Parameters |\n",
      "+----------------------------------------+------------+\n",
      "|            embedding.weight            |  38597376  |\n",
      "|      positional_embedding.weight       |   786432   |\n",
      "|       transformers.0.ln_1.weight       |    768     |\n",
      "|        transformers.0.ln_1.bias        |    768     |\n",
      "| transformers.0.attention.q_mat.weight  |   589824   |\n",
      "| transformers.0.attention.k_mat.weight  |   589824   |\n",
      "| transformers.0.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.0.attention.out.weight   |   589824   |\n",
      "|   transformers.0.attention.out.bias    |    768     |\n",
      "|       transformers.0.ln_2.weight       |    768     |\n",
      "|        transformers.0.ln_2.bias        |    768     |\n",
      "|    transformers.0.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.0.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.0.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.0.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.1.ln_1.weight       |    768     |\n",
      "|        transformers.1.ln_1.bias        |    768     |\n",
      "| transformers.1.attention.q_mat.weight  |   589824   |\n",
      "| transformers.1.attention.k_mat.weight  |   589824   |\n",
      "| transformers.1.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.1.attention.out.weight   |   589824   |\n",
      "|   transformers.1.attention.out.bias    |    768     |\n",
      "|       transformers.1.ln_2.weight       |    768     |\n",
      "|        transformers.1.ln_2.bias        |    768     |\n",
      "|    transformers.1.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.1.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.1.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.1.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.2.ln_1.weight       |    768     |\n",
      "|        transformers.2.ln_1.bias        |    768     |\n",
      "| transformers.2.attention.q_mat.weight  |   589824   |\n",
      "| transformers.2.attention.k_mat.weight  |   589824   |\n",
      "| transformers.2.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.2.attention.out.weight   |   589824   |\n",
      "|   transformers.2.attention.out.bias    |    768     |\n",
      "|       transformers.2.ln_2.weight       |    768     |\n",
      "|        transformers.2.ln_2.bias        |    768     |\n",
      "|    transformers.2.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.2.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.2.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.2.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.3.ln_1.weight       |    768     |\n",
      "|        transformers.3.ln_1.bias        |    768     |\n",
      "| transformers.3.attention.q_mat.weight  |   589824   |\n",
      "| transformers.3.attention.k_mat.weight  |   589824   |\n",
      "| transformers.3.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.3.attention.out.weight   |   589824   |\n",
      "|   transformers.3.attention.out.bias    |    768     |\n",
      "|       transformers.3.ln_2.weight       |    768     |\n",
      "|        transformers.3.ln_2.bias        |    768     |\n",
      "|    transformers.3.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.3.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.3.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.3.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.4.ln_1.weight       |    768     |\n",
      "|        transformers.4.ln_1.bias        |    768     |\n",
      "| transformers.4.attention.q_mat.weight  |   589824   |\n",
      "| transformers.4.attention.k_mat.weight  |   589824   |\n",
      "| transformers.4.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.4.attention.out.weight   |   589824   |\n",
      "|   transformers.4.attention.out.bias    |    768     |\n",
      "|       transformers.4.ln_2.weight       |    768     |\n",
      "|        transformers.4.ln_2.bias        |    768     |\n",
      "|    transformers.4.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.4.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.4.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.4.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.5.ln_1.weight       |    768     |\n",
      "|        transformers.5.ln_1.bias        |    768     |\n",
      "| transformers.5.attention.q_mat.weight  |   589824   |\n",
      "| transformers.5.attention.k_mat.weight  |   589824   |\n",
      "| transformers.5.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.5.attention.out.weight   |   589824   |\n",
      "|   transformers.5.attention.out.bias    |    768     |\n",
      "|       transformers.5.ln_2.weight       |    768     |\n",
      "|        transformers.5.ln_2.bias        |    768     |\n",
      "|    transformers.5.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.5.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.5.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.5.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.6.ln_1.weight       |    768     |\n",
      "|        transformers.6.ln_1.bias        |    768     |\n",
      "| transformers.6.attention.q_mat.weight  |   589824   |\n",
      "| transformers.6.attention.k_mat.weight  |   589824   |\n",
      "| transformers.6.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.6.attention.out.weight   |   589824   |\n",
      "|   transformers.6.attention.out.bias    |    768     |\n",
      "|       transformers.6.ln_2.weight       |    768     |\n",
      "|        transformers.6.ln_2.bias        |    768     |\n",
      "|    transformers.6.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.6.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.6.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.6.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.7.ln_1.weight       |    768     |\n",
      "|        transformers.7.ln_1.bias        |    768     |\n",
      "| transformers.7.attention.q_mat.weight  |   589824   |\n",
      "| transformers.7.attention.k_mat.weight  |   589824   |\n",
      "| transformers.7.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.7.attention.out.weight   |   589824   |\n",
      "|   transformers.7.attention.out.bias    |    768     |\n",
      "|       transformers.7.ln_2.weight       |    768     |\n",
      "|        transformers.7.ln_2.bias        |    768     |\n",
      "|    transformers.7.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.7.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.7.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.7.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.8.ln_1.weight       |    768     |\n",
      "|        transformers.8.ln_1.bias        |    768     |\n",
      "| transformers.8.attention.q_mat.weight  |   589824   |\n",
      "| transformers.8.attention.k_mat.weight  |   589824   |\n",
      "| transformers.8.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.8.attention.out.weight   |   589824   |\n",
      "|   transformers.8.attention.out.bias    |    768     |\n",
      "|       transformers.8.ln_2.weight       |    768     |\n",
      "|        transformers.8.ln_2.bias        |    768     |\n",
      "|    transformers.8.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.8.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.8.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.8.MLP.out_ff.bias     |    768     |\n",
      "|       transformers.9.ln_1.weight       |    768     |\n",
      "|        transformers.9.ln_1.bias        |    768     |\n",
      "| transformers.9.attention.q_mat.weight  |   589824   |\n",
      "| transformers.9.attention.k_mat.weight  |   589824   |\n",
      "| transformers.9.attention.v_mat.weight  |   589824   |\n",
      "|  transformers.9.attention.out.weight   |   589824   |\n",
      "|   transformers.9.attention.out.bias    |    768     |\n",
      "|       transformers.9.ln_2.weight       |    768     |\n",
      "|        transformers.9.ln_2.bias        |    768     |\n",
      "|    transformers.9.MLP.in_ff.weight     |  2359296   |\n",
      "|     transformers.9.MLP.in_ff.bias      |    3072    |\n",
      "|    transformers.9.MLP.out_ff.weight    |  2359296   |\n",
      "|     transformers.9.MLP.out_ff.bias     |    768     |\n",
      "|      transformers.10.ln_1.weight       |    768     |\n",
      "|       transformers.10.ln_1.bias        |    768     |\n",
      "| transformers.10.attention.q_mat.weight |   589824   |\n",
      "| transformers.10.attention.k_mat.weight |   589824   |\n",
      "| transformers.10.attention.v_mat.weight |   589824   |\n",
      "|  transformers.10.attention.out.weight  |   589824   |\n",
      "|   transformers.10.attention.out.bias   |    768     |\n",
      "|      transformers.10.ln_2.weight       |    768     |\n",
      "|       transformers.10.ln_2.bias        |    768     |\n",
      "|    transformers.10.MLP.in_ff.weight    |  2359296   |\n",
      "|     transformers.10.MLP.in_ff.bias     |    3072    |\n",
      "|   transformers.10.MLP.out_ff.weight    |  2359296   |\n",
      "|    transformers.10.MLP.out_ff.bias     |    768     |\n",
      "|      transformers.11.ln_1.weight       |    768     |\n",
      "|       transformers.11.ln_1.bias        |    768     |\n",
      "| transformers.11.attention.q_mat.weight |   589824   |\n",
      "| transformers.11.attention.k_mat.weight |   589824   |\n",
      "| transformers.11.attention.v_mat.weight |   589824   |\n",
      "|  transformers.11.attention.out.weight  |   589824   |\n",
      "|   transformers.11.attention.out.bias   |    768     |\n",
      "|      transformers.11.ln_2.weight       |    768     |\n",
      "|       transformers.11.ln_2.bias        |    768     |\n",
      "|    transformers.11.MLP.in_ff.weight    |  2359296   |\n",
      "|     transformers.11.MLP.in_ff.bias     |    3072    |\n",
      "|   transformers.11.MLP.out_ff.weight    |  2359296   |\n",
      "|    transformers.11.MLP.out_ff.bias     |    768     |\n",
      "|               ln.weight                |    768     |\n",
      "|                ln.bias                 |    768     |\n",
      "|             output.weight              |  38597376  |\n",
      "+----------------------------------------+------------+\n",
      "Total Trainable Params: 163009536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "163009536"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "gpt = GPT(\n",
    "    vocab_size=50257,\n",
    "    context_length=1024,\n",
    "    emb_dim=768,\n",
    "    ff_int_dim_mult=4,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    drop_rate=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n",
      "input:  torch.Size([10, 1024])\n",
      "output:  torch.Size([10, 1024])\n",
      "paddings:  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from train import TinyStoriesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "ds = TinyStoriesDataset('/teamspace/studios/this_studio/transformers/data/TinyStoriesV2-GPT4-train.txt', 1024, tokenizer, end_story_idx=150)\n",
    "dl = DataLoader(ds, batch_size=10, shuffle=True)\n",
    "\n",
    "for input, target, paddings in dl:\n",
    "    print(\"input: \", input.shape)\n",
    "    print(\"output: \", target.shape)\n",
    "    print(\"paddings: \", paddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.0185, grad_fn=<MeanBackward0>)\n",
      "tensor(-12.1852, grad_fn=<MeanBackward0>)\n",
      "tensor(-12.7005, grad_fn=<MeanBackward0>)\n",
      "tensor(-13.2183, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      5\u001b[0m gpt \u001b[38;5;241m=\u001b[39m GPT(\n\u001b[1;32m      6\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50257\u001b[39m,\n\u001b[1;32m      7\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     qkv_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrain_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/train.py:238\u001b[0m, in \u001b[0;36mtrain_gpt\u001b[0;34m(model, batch_size, num_epochs, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m, target, paddings \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[1;32m    237\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     padding_tokens_removed_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    241\u001b[0m     padding_tokens_removed_target \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers/train.py:155\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    153\u001b[0m     batches, context_length \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 155\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     positional_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding(torch\u001b[38;5;241m.\u001b[39marange(context_length))\n\u001b[1;32m    158\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m token_embeddings \u001b[38;5;241m+\u001b[39m positional_embeddings\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from train import train_gpt, GPT\n",
    "\n",
    "torch.manual_seed(123)\n",
    "gpt = GPT(\n",
    "    vocab_size=50257,\n",
    "    context_length=128,\n",
    "    emb_dim=768,\n",
    "    ff_int_dim_mult=4,\n",
    "    n_heads=12,\n",
    "    n_layers=12,\n",
    "    drop_rate=0.1,\n",
    "    qkv_bias=False\n",
    ")\n",
    "\n",
    "train_gpt(gpt, batch_size=2, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
